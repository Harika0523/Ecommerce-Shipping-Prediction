# -*- coding: utf-8 -*-
"""E-Commerce_Shipping_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/167AzfVRjIF1tylJqgCgXuVmfyzxwvwAG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle as pkl
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import Normalizer
from sklearn.linear_model import LogisticRegression, RidgeClassifier, LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

"""# **Collecting Data**"""

data = pd.read_csv("/content/Train.csv")

data.head()

"""# **Data Preprocessing**"""

data.shape

data.info()

"""Handling missing values"""

# Check for missing values
print(data.isnull().sum())

"""Handling Categorical Values"""

categorical_cols = data.select_dtypes(include=['object']).columns
print("Categorical columns:", categorical_cols)

label_encoder = LabelEncoder()

# Apply LabelEncoder to each categorical column
for col in categorical_cols:
    data[col] = label_encoder.fit_transform(data[col])

"""Handling Outliers"""

# Visualize outliers using boxplot
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns

num_cols = 3  # Number of plots per row
num_plots = len(numerical_cols)

plt.figure(figsize=(15, 5 * (num_plots // num_cols + 1)))  # Adjust figure size based on number of plots

for i, col in enumerate(numerical_cols):
    plt.subplot((num_plots // num_cols) + 1, num_cols, i + 1)
    sns.boxplot(x=data[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

# Calculate upper and lower bounds using IQR
def calculate_bounds(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return lower_bound, upper_bound

# Apply transformation technique to handle outliers (e.g., clipping)
def clip_outliers(data, col):
    lower_bound, upper_bound = calculate_bounds(data, col)
    data[col] = np.clip(data[col], lower_bound, upper_bound)

# Apply clipping to all numerical columns
for col in numerical_cols:
    clip_outliers(data, col)

# Display the first few rows of the transformed dataset
data.head()

"""# **Descriptive Statistics**"""

data.describe()

"""# **Visual Analysis**

Univariate analysis
"""

numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns
numerical_columns = numerical_columns.drop('ID', errors='ignore')

# Number of plots per row
num_cols = 3
num_rows = -(-len(numerical_columns) // num_cols)  # Ceiling division

plt.figure(figsize=(5 * num_cols, 5 * num_rows))

for i, column in enumerate(numerical_columns):
    plt.subplot(num_rows, num_cols, i + 1)
    sns.histplot(data[column], bins=30, kde=True)
    plt.title(f'Distribution of {column}')
    plt.xlabel('')
    plt.ylabel('')

plt.tight_layout()
plt.show()

"""Bivariate analysis"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the figure size for all plots
plt.figure(figsize=(18, 12))

# Plot 1: Warehouse Block vs. Reached on Time
plt.subplot(2, 2, 1)
sns.countplot(x='Warehouse_block', hue='Reached.on.Time_Y.N', data=data, palette='viridis')
plt.title('Warehouse Block vs. Reached on Time')
plt.xlabel('Warehouse Block')
plt.ylabel('Count')

# Plot 2: Mode of Shipment vs. Reached on Time
plt.subplot(2, 2, 2)
sns.countplot(x='Mode_of_Shipment', hue='Reached.on.Time_Y.N', data=data, palette='viridis')
plt.title('Mode of Shipment vs. Reached on Time')
plt.xlabel('Mode of Shipment')
plt.ylabel('Count')

# Plot 3: Product Importance vs. Reached on Time
plt.subplot(2, 2, 3)
sns.countplot(x='Product_importance', hue='Reached.on.Time_Y.N', data=data, palette='viridis')
plt.title('Product Importance vs. Reached on Time')
plt.xlabel('Product Importance')
plt.ylabel('Count')

# Plot 4: Gender vs. Reached on Time
plt.subplot(2, 2, 4)
sns.countplot(x='Gender', hue='Reached.on.Time_Y.N', data=data, palette='viridis')
plt.title('Gender vs. Reached on Time')
plt.xlabel('Gender')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

"""Multivariate analysis"""

# Compute the correlation matrix
corr_matrix = data.corr()

# Set up the matplotlib figure
plt.figure(figsize=(12, 10))

# Draw the heatmap with the correlation matrix
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, annot_kws={"size": 10})

# Title and labels
plt.title('Correlation Heatmap', size=15)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.show()

"""# **Splitting Data**"""

# Separate the features and the target variable
x = data.drop(columns=['ID', 'Reached.on.Time_Y.N'])
y = data['Reached.on.Time_Y.N']

# Split the dataset into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

print(f"X_train shape: {x_train.shape}")
print(f"X_test shape: {x_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Initialize the Normalizer with norm='l1'
normalizer = Normalizer(norm='l1')

# Apply normalization to x_train and x_test
x_train_normalized = normalizer.fit_transform(x_train)
x_test_normalized = normalizer.transform(x_test)

print(x_train_normalized.shape)
print(x_test_normalized.shape)

"""# **Training and Testing the model**"""

def models_eval(x_train, y_train, x_test, y_test):
    models = {
        'Logistic Regression': LogisticRegression(random_state=1234),
        'Logistic Regression CV': LogisticRegressionCV(random_state=1234),
        'XGBoost': XGBClassifier(random_state=1234),
        'Ridge Classifier': RidgeClassifier(random_state=1234),
        'KNN': KNeighborsClassifier(),
        'Random Forest': RandomForestClassifier(random_state=1234),
        'SVM Classifier': svm.SVC(random_state=1234)
    }

    for name, model in models.items():
        model.fit(x_train, y_train)
        train_score = model.score(x_train, y_train)
        test_score = model.score(x_test, y_test)
        print(f' -- {name}')
        print(f'Train Score: {train_score:.4f}')
        print(f'Test Score: {test_score:.4f}')
        print()

    return models

models = models_eval(x_train_normalized, y_train, x_test_normalized, y_test)

# Testing each model with the first instance in the test set
test_instance = x_test_normalized[0].reshape(1, -1)

for name, model in models.items():
    predicted_label = model.predict(test_instance)
    print(f'{name} Prediction:', predicted_label[0])

"""# **Testing Model With Multiple Evaluation Metrics**"""

# Define a function to evaluate models
def eval_model(model, x_test, y_test):
    y_pred = model.predict(x_test)
    return [
        "{:.2f}".format(accuracy_score(y_test, y_pred) * 100),
        "{:.2f}".format(f1_score(y_test, y_pred) * 100),
        "{:.2f}".format(recall_score(y_test, y_pred) * 100),
        "{:.2f}".format(precision_score(y_test, y_pred) * 100)
    ]

# Evaluate models and store the results
model_eval_info = [
    [name] + eval_model(model, x_test_normalized, y_test)
    for name, model in models.items()
]

# Create DataFrame and save to CSV
model_eval_df = pd.DataFrame(model_eval_info, columns=['Name', 'Accuracy', 'F1 Score', 'Recall', 'Precision'])
model_eval_df.to_csv('model_eval.csv', index=False)
print(model_eval_df)



"""Logistic Regression"""

# Train and evaluate Logistic Regression model
logistic_regression = LogisticRegression(random_state=1234)
logistic_regression.fit(x_train_normalized, y_train)
y_pred_lr = logistic_regression.predict(x_test_normalized)

# Classification report and confusion matrix for Logistic Regression
print("Logistic Regression Classification Report")
print(classification_report(y_test, y_pred_lr))

confusion_matrix(y_test, y_pred_lr)

"""LogisticRegressionCV"""

logistic_regression_cv = LogisticRegressionCV(random_state=1234)
logistic_regression_cv.fit(x_train_normalized, y_train)
y_pred_lrcv = logistic_regression_cv.predict(x_test_normalized)

# Classification report and confusion matrix for Logistic Regression CV
print("Logistic Regression CV Classification Report")
print(classification_report(y_test, y_pred_lrcv))

confusion_matrix(y_test, y_pred_lrcv)

"""XGBoost"""

xgboost = XGBClassifier(random_state=1234)
xgboost.fit(x_train_normalized, y_train)
y_pred_xgb = xgboost.predict(x_test_normalized)

# Classification report and confusion matrix for XGBoost
print("XGBoost Classification Report")
print(classification_report(y_test, y_pred_xgb))

confusion_matrix(y_test, y_pred_xgb)

"""Ridge Classifier"""

ridge_classifier = RidgeClassifier(random_state=1234)
ridge_classifier.fit(x_train_normalized, y_train)
y_pred_rc = ridge_classifier.predict(x_test_normalized)

# Classification report and confusion matrix for Ridge Classifier
print("Ridge Classifier Classification Report")
print(classification_report(y_test, y_pred_rc))

confusion_matrix(y_test, y_pred_rc)

"""K-Nearest Neighbors (KNN)"""

knn = KNeighborsClassifier()
knn.fit(x_train_normalized, y_train)
y_pred_knn = knn.predict(x_test_normalized)

# Classification report and confusion matrix for KNN
print("KNN Classification Report")
print(classification_report(y_test, y_pred_knn))

confusion_matrix(y_test, y_pred_knn)

"""Random Forest"""

rf_model = RandomForestClassifier(random_state=1234)
rf_model.fit(x_train_normalized, y_train)

# Predict and evaluate the model
y_pred_rf = rf_model.predict(x_test_normalized)
print("Random Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

confusion_matrix(y_test, y_pred_rf)

"""SVM Classifier"""

svm_model = svm.SVC(random_state=1234)
svm_model.fit(x_train_normalized, y_train)

# Predict and evaluate the model
y_pred_svm = svm_model.predict(x_test_normalized)
print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

confusion_matrix(y_test, y_pred_svm)

"""# **Hyperparameter Tuning**"""

# Define the parameter grid for SVC
svc_param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Initialize and fit GridSearchCV for SVC
svc_grid = GridSearchCV(SVC(random_state=1234), svc_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
svc_grid.fit(x_train_normalized, y_train)

# Print best parameters and score
print("Best parameters for SVC:", svc_grid.best_params_)
print("Best score for SVC:", svc_grid.best_score_)

"""Random Forest"""

from sklearn.model_selection import RandomizedSearchCV

# Define the parameter grid for Random Forest
rf_param_distributions = {
    'n_estimators': [50, 100, 200],
    'criterion': ['gini', 'entropy'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize and fit RandomizedSearchCV for Random Forest
rf_random = RandomizedSearchCV(RandomForestClassifier(random_state=1234), rf_param_distributions, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, verbose=2, random_state=1234)
rf_random.fit(x_train_normalized, y_train)

# Print best parameters and score
print("Best parameters for Random Forest:", rf_random.best_params_)
print("Best score for Random Forest:", rf_random.best_score_)

"""XGBoost"""

# Define the parameter grid for XGBoost
xgb_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 0.8, 0.9]
}

# Initialize and fit GridSearchCV for XGBoost
xgb_grid = GridSearchCV(XGBClassifier(random_state=1234), xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
xgb_grid.fit(x_train_normalized, y_train)

# Print best parameters and score
print("Best parameters for XGBoost:", xgb_grid.best_params_)
print("Best score for XGBoost:", xgb_grid.best_score_)

"""Logistic Regression CV"""

# Define the parameter grid for Logistic Regression CV
log_reg_cv_param_grid = {
    'Cs': [1, 10, 100],
    'cv': [5, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']
}

# Initialize and fit GridSearchCV for Logistic Regression CV
log_reg_cv_grid = GridSearchCV(LogisticRegressionCV(random_state=1234), log_reg_cv_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
log_reg_cv_grid.fit(x_train_normalized, y_train)

# Print best parameters and score
print("Best parameters for Logistic Regression CV:", log_reg_cv_grid.best_params_)
print("Best score for Logistic Regression CV:", log_reg_cv_grid.best_score_)

"""# **Saving the best model**"""

import pickle as pkl

# Find the best model based on accuracy
best_model_name = max(model_eval_info, key=lambda x: float(x[1]))[0]
best_model = models[best_model_name]
print(f'Best Model: {best_model_name}')

# Save the best model and the normalizer
pkl.dump(best_model, open(f'{best_model_name}_best_model.pkl', 'wb'))
pkl.dump(normalizer, open('normalizer.pkl', 'wb'))

from google.colab import files

files.download(f'{best_model_name}_best_model.pkl')
files.download('normalizer.pkl')

import pandas as pd
import pickle
from sklearn.preprocessing import LabelEncoder

# Load your dataset
data = pd.read_csv("/content/Train.csv")

# Initialize LabelEncoders for each categorical column
label_encoders = {
    'Warehouse_block': LabelEncoder(),
    'Mode_of_Shipment': LabelEncoder(),
    'Product_importance': LabelEncoder(),
    'Gender': LabelEncoder()
}

# Fit the LabelEncoders on the categorical columns
for column, encoder in label_encoders.items():
    encoder.fit(data[column])
    # Save each LabelEncoder as a .pkl file
    with open(f'label_encoder_{column.lower()}.pkl', 'wb') as file:
        pickle.dump(encoder, file)

print("Label encoders saved successfully.")

